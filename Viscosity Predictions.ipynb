{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data review - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DEPTH (FT AH)</th>\n",
       "      <th>SG (Oil Gravity) @ 60/60F</th>\n",
       "      <th>API</th>\n",
       "      <th>Viscosity @ SC, cSt</th>\n",
       "      <th>Temp (deg F, res)</th>\n",
       "      <th>Viscosity @ RC, cp</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Uob, cp</th>\n",
       "      <th>Uo, cp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.8</td>\n",
       "      <td>121.00</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.8</td>\n",
       "      <td>112.60</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.3</td>\n",
       "      <td>127.30</td>\n",
       "      <td>126</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6994</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.0</td>\n",
       "      <td>114.60</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6994</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.9</td>\n",
       "      <td>113.90</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9820</td>\n",
       "      <td>0.91</td>\n",
       "      <td>23.3</td>\n",
       "      <td>17.32</td>\n",
       "      <td>171</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4200</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9820</td>\n",
       "      <td>0.92</td>\n",
       "      <td>22.5</td>\n",
       "      <td>20.04</td>\n",
       "      <td>171</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4200</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10544</td>\n",
       "      <td>0.82</td>\n",
       "      <td>40.2</td>\n",
       "      <td>2.82</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>2.1</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10135</td>\n",
       "      <td>0.83</td>\n",
       "      <td>38.9</td>\n",
       "      <td>2.86</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10551</td>\n",
       "      <td>0.83</td>\n",
       "      <td>39.2</td>\n",
       "      <td>8.94</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  DEPTH (FT AH)  SG (Oil Gravity) @ 60/60F   API  \\\n",
       "0           NaN           6989                       0.95  17.8   \n",
       "1           NaN           6989                       0.95  17.8   \n",
       "2           NaN           6989                       0.95  18.3   \n",
       "3           NaN           6994                       0.95  18.0   \n",
       "4           NaN           6994                       0.95  17.9   \n",
       "..          ...            ...                        ...   ...   \n",
       "357         NaN           9820                       0.91  23.3   \n",
       "358         NaN           9820                       0.92  22.5   \n",
       "359         NaN          10544                       0.82  40.2   \n",
       "360         NaN          10135                       0.83  38.9   \n",
       "361         NaN          10551                       0.83  39.2   \n",
       "\n",
       "     Viscosity @ SC, cSt  Temp (deg F, res)  Viscosity @ RC, cp  Pressure  \\\n",
       "0                 121.00                126                13.0      2890   \n",
       "1                 112.60                126                13.0      2890   \n",
       "2                 127.30                126                14.0      2890   \n",
       "3                 114.60                126                13.0      2890   \n",
       "4                 113.90                126                13.0      2890   \n",
       "..                   ...                ...                 ...       ...   \n",
       "357                17.32                171                 5.1      4200   \n",
       "358                20.04                171                 5.9      4200   \n",
       "359                 2.82                191                 0.9      4500   \n",
       "360                 2.86                191                 0.9      4500   \n",
       "361                 8.94                191                 0.9      4500   \n",
       "\n",
       "     Uob, cp  Uo, cp  \n",
       "0       14.0    14.0  \n",
       "1       14.0    14.0  \n",
       "2       15.0    15.0  \n",
       "3       14.0    14.0  \n",
       "4       14.0    14.0  \n",
       "..       ...     ...  \n",
       "357     11.0    25.0  \n",
       "358     12.0    28.0  \n",
       "359      2.1     6.7  \n",
       "360      1.0     1.2  \n",
       "361      2.0    12.8  \n",
       "\n",
       "[362 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'DEPTH (FT AH)', 'SG (Oil Gravity) @ 60/60F', 'API',\n",
       "       'Viscosity @ SC, cSt', 'Temp (deg F, res)', 'Viscosity @ RC, cp',\n",
       "       'Pressure', 'Uob, cp', 'Uo, cp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'Uo, cp'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH (FT AH)</th>\n",
       "      <th>SG (Oil Gravity) @ 60/60F</th>\n",
       "      <th>API</th>\n",
       "      <th>Viscosity @ SC, cSt</th>\n",
       "      <th>Temp (deg F, res)</th>\n",
       "      <th>Viscosity @ RC, cp</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Uob, cp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.8</td>\n",
       "      <td>121.00</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.8</td>\n",
       "      <td>112.60</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6989</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.3</td>\n",
       "      <td>127.30</td>\n",
       "      <td>126</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6994</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.0</td>\n",
       "      <td>114.60</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6994</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17.9</td>\n",
       "      <td>113.90</td>\n",
       "      <td>126</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>9820</td>\n",
       "      <td>0.91</td>\n",
       "      <td>23.3</td>\n",
       "      <td>17.32</td>\n",
       "      <td>171</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4200</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>9820</td>\n",
       "      <td>0.92</td>\n",
       "      <td>22.5</td>\n",
       "      <td>20.04</td>\n",
       "      <td>171</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4200</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>10544</td>\n",
       "      <td>0.82</td>\n",
       "      <td>40.2</td>\n",
       "      <td>2.82</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>10135</td>\n",
       "      <td>0.83</td>\n",
       "      <td>38.9</td>\n",
       "      <td>2.86</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>10551</td>\n",
       "      <td>0.83</td>\n",
       "      <td>39.2</td>\n",
       "      <td>8.94</td>\n",
       "      <td>191</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DEPTH (FT AH)  SG (Oil Gravity) @ 60/60F   API  Viscosity @ SC, cSt  \\\n",
       "0             6989                       0.95  17.8               121.00   \n",
       "1             6989                       0.95  17.8               112.60   \n",
       "2             6989                       0.95  18.3               127.30   \n",
       "3             6994                       0.95  18.0               114.60   \n",
       "4             6994                       0.95  17.9               113.90   \n",
       "..             ...                        ...   ...                  ...   \n",
       "357           9820                       0.91  23.3                17.32   \n",
       "358           9820                       0.92  22.5                20.04   \n",
       "359          10544                       0.82  40.2                 2.82   \n",
       "360          10135                       0.83  38.9                 2.86   \n",
       "361          10551                       0.83  39.2                 8.94   \n",
       "\n",
       "     Temp (deg F, res)  Viscosity @ RC, cp  Pressure  Uob, cp  \n",
       "0                  126                13.0      2890     14.0  \n",
       "1                  126                13.0      2890     14.0  \n",
       "2                  126                14.0      2890     15.0  \n",
       "3                  126                13.0      2890     14.0  \n",
       "4                  126                13.0      2890     14.0  \n",
       "..                 ...                 ...       ...      ...  \n",
       "357                171                 5.1      4200     11.0  \n",
       "358                171                 5.9      4200     12.0  \n",
       "359                191                 0.9      4500      2.1  \n",
       "360                191                 0.9      4500      1.0  \n",
       "361                191                 0.9      4500      2.0  \n",
       "\n",
       "[362 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_indices(random_state, bootstrap, n_population, n_samples):\n",
    "    \"\"\"Draw randomly sampled indices.\"\"\"\n",
    "    # Draw sample indices\n",
    "    if bootstrap:\n",
    "        indices = random_state.randint(0, n_population, n_samples)\n",
    "    else:\n",
    "        indices = sample_without_replacement(n_population, n_samples,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _generate_bagging_indices(random_state, bootstrap_features,\n",
    "                              bootstrap_samples, n_features, n_samples,\n",
    "                              max_features, max_samples):\n",
    "    \"\"\"Randomly draw feature and sample indices.\"\"\"\n",
    "    # Get valid random state\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # Draw indices\n",
    "    feature_indices = _generate_indices(random_state, bootstrap_features,\n",
    "                                        n_features, max_features)\n",
    "    sample_indices = _generate_indices(random_state, bootstrap_samples,\n",
    "                                       n_samples, max_samples)\n",
    "\n",
    "    return feature_indices, sample_indices\n",
    "\n",
    "\n",
    "def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,\n",
    "                               seeds, total_n_estimators, verbose):\n",
    "    \"\"\"Private function used to build a batch of estimators within a job.\"\"\"\n",
    "    # Retrieve settings\n",
    "    n_samples, n_features = X.shape\n",
    "    max_features = ensemble._max_features\n",
    "    max_samples = ensemble._max_samples\n",
    "    bootstrap = ensemble.bootstrap\n",
    "    bootstrap_features = ensemble.bootstrap_features\n",
    "    support_sample_weight = has_fit_parameter(ensemble.base_estimator_,\n",
    "                                              \"sample_weight\")\n",
    "    if not support_sample_weight and sample_weight is not None:\n",
    "        raise ValueError(\"The base estimator doesn't support sample weight\")\n",
    "\n",
    "    # Build estimators\n",
    "    estimators = []\n",
    "    estimators_features = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        if verbose > 1:\n",
    "            print(\"Building estimator %d of %d for this parallel run \"\n",
    "                  \"(total %d)...\" % (i + 1, n_estimators, total_n_estimators))\n",
    "\n",
    "        random_state = seeds[i]\n",
    "        estimator = ensemble._make_estimator(append=False,\n",
    "                                             random_state=random_state)\n",
    "\n",
    "        # Draw random feature, sample indices\n",
    "        features, indices = _generate_bagging_indices(random_state,\n",
    "                                                      bootstrap_features,\n",
    "                                                      bootstrap, n_features,\n",
    "                                                      n_samples, max_features,\n",
    "                                                      max_samples)\n",
    "\n",
    "        # Draw samples, using sample weights, and then fit\n",
    "        if support_sample_weight:\n",
    "            if sample_weight is None:\n",
    "                curr_sample_weight = np.ones((n_samples,))\n",
    "            else:\n",
    "                curr_sample_weight = sample_weight.copy()\n",
    "\n",
    "            if bootstrap:\n",
    "                sample_counts = np.bincount(indices, minlength=n_samples)\n",
    "                curr_sample_weight *= sample_counts\n",
    "            else:\n",
    "                not_indices_mask = ~indices_to_mask(indices, n_samples)\n",
    "                curr_sample_weight[not_indices_mask] = 0\n",
    "\n",
    "            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n",
    "\n",
    "        else:\n",
    "            estimator.fit((X[indices])[:, features], y[indices])\n",
    "\n",
    "        estimators.append(estimator)\n",
    "        estimators_features.append(features)\n",
    "\n",
    "    return estimators, estimators_features\n",
    "\n",
    "\n",
    "def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    proba = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            proba_estimator = estimator.predict_proba(X[:, features])\n",
    "\n",
    "            if n_classes == len(estimator.classes_):\n",
    "                proba += proba_estimator\n",
    "\n",
    "            else:\n",
    "                proba[:, estimator.classes_] += \\\n",
    "                    proba_estimator[:, range(len(estimator.classes_))]\n",
    "\n",
    "        else:\n",
    "            # Resort to voting\n",
    "            predictions = estimator.predict(X[:, features])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                proba[i, predictions[i]] += 1\n",
    "\n",
    "    return proba\n",
    "\n",
    "\n",
    "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    log_proba = np.empty((n_samples, n_classes))\n",
    "    log_proba.fill(-np.inf)\n",
    "    all_classes = np.arange(n_classes, dtype=np.int)\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
    "\n",
    "        if n_classes == len(estimator.classes_):\n",
    "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
    "\n",
    "        else:\n",
    "            log_proba[:, estimator.classes_] = np.logaddexp(\n",
    "                log_proba[:, estimator.classes_],\n",
    "                log_proba_estimator[:, range(len(estimator.classes_))])\n",
    "\n",
    "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
    "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
    "                                                 -np.inf)\n",
    "\n",
    "    return log_proba\n",
    "\n",
    "\n",
    "def _parallel_decision_function(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
    "    return sum(estimator.decision_function(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n",
    "\n",
    "\n",
    "def _parallel_predict_regression(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute predictions within a job.\"\"\"\n",
    "    return sum(estimator.predict(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
    "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    log_proba = np.empty((n_samples, n_classes))\n",
    "    log_proba.fill(-np.inf)\n",
    "    all_classes = np.arange(n_classes, dtype=np.int)\n",
    "\n",
    "    for estimator, features in zip(estimators, estimators_features):\n",
    "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
    "\n",
    "        if n_classes == len(estimator.classes_):\n",
    "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
    "\n",
    "        else:\n",
    "            log_proba[:, estimator.classes_] = np.logaddexp(\n",
    "                log_proba[:, estimator.classes_],\n",
    "                log_proba_estimator[:, range(len(estimator.classes_))])\n",
    "\n",
    "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
    "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
    "                                                 -np.inf)\n",
    "\n",
    "    return log_proba\n",
    "\n",
    "\n",
    "def _parallel_decision_function(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
    "    return sum(estimator.decision_function(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n",
    "\n",
    "\n",
    "def _parallel_predict_regression(estimators, estimators_features, X):\n",
    "    \"\"\"Private function used to compute predictions within a job.\"\"\"\n",
    "    return sum(estimator.predict(X[:, features])\n",
    "               for estimator, features in zip(estimators,\n",
    "                                              estimators_features))\n",
    "\n",
    "\n",
    "class BaseBagging(BaseEnsemble, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for Bagging meta-estimator.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=False,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=None,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators)\n",
    "\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.oob_score = oob_score\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
    "\n",
    "    def _parallel_args(self):\n",
    "        return {}\n",
    "\n",
    "    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n",
    "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
    "           set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        max_samples : int or float, default=None\n",
    "            Argument to use instead of self.max_samples.\n",
    "        max_depth : int, default=None\n",
    "            Override value used when constructing base estimator. Only\n",
    "            supported if the base estimator has a max_depth parameter.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Note that this is supported only if the base estimator supports\n",
    "            sample weighting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # Convert data (X is required to be 2d and indexable)\n",
    "        X, y = self._validate_data(\n",
    "            X, y, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False, multi_output=True\n",
    "        )\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)\n",
    "\n",
    "        # Remap output\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        self._n_samples = n_samples\n",
    "        y = self._validate_y(y)\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        if max_depth is not None:\n",
    "            self.base_estimator_.max_depth = max_depth\n",
    "\n",
    "        # Validate max_samples\n",
    "        if max_samples is None:\n",
    "            max_samples = self.max_samples\n",
    "        elif not isinstance(max_samples, numbers.Integral):\n",
    "            max_samples = int(max_samples * X.shape[0])\n",
    "\n",
    "        if not (0 < max_samples <= X.shape[0]):\n",
    "            raise ValueError(\"max_samples must be in (0, n_samples]\")\n",
    "\n",
    "        # Store validated integer row sampling value\n",
    "        self._max_samples = max_samples\n",
    "\n",
    "        # Validate max_features\n",
    "        if isinstance(self.max_features, numbers.Integral):\n",
    "            max_features = self.max_features\n",
    "        elif isinstance(self.max_features, np.float):\n",
    "            max_features = self.max_features * self.n_features_\n",
    "        else:\n",
    "            raise ValueError(\"max_features must be int or float\")\n",
    "\n",
    "        if not (0 < max_features <= self.n_features_):\n",
    "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
    "\n",
    "        max_features = max(1, int(max_features))\n",
    "\n",
    "        # Store validated integer feature sampling value\n",
    "        self._max_features = max_features\n",
    "\n",
    "        # Other checks\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available\"\n",
    "                             \" if bootstrap=True\")\n",
    "\n",
    "        if self.warm_start and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimate only available\"\n",
    "                             \" if warm_start=False\")\n",
    "\n",
    "        if hasattr(self, \"oob_score_\") and self.warm_start:\n",
    "            del self.oob_score_\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, 'estimators_'):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "            self.estimators_features_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:\n",
    "            raise ValueError('n_estimators=%d must be larger or equal to '\n",
    "                             'len(estimators_)=%d when warm_start==True'\n",
    "                             % (self.n_estimators, len(self.estimators_)))\n",
    "\n",
    "        elif n_more_estimators == 0:\n",
    "            warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
    "                 \"fit new trees.\")\n",
    "            return self\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n",
    "                                                             self.n_jobs)\n",
    "        total_n_estimators = sum(n_estimators)\n",
    "\n",
    "        # Advance random state to state after training\n",
    "        # the first n_estimators\n",
    "        if self.warm_start and len(self.estimators_) > 0:\n",
    "            random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n",
    "        self._seeds = seeds\n",
    "\n",
    "        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
    "                               **self._parallel_args())(\n",
    "            delayed(_parallel_build_estimators)(\n",
    "                n_estimators[i],\n",
    "                self,\n",
    "                X,\n",
    "                y,\n",
    "                sample_weight,\n",
    "                seeds[starts[i]:starts[i + 1]],\n",
    "                total_n_estimators,\n",
    "                verbose=self.verbose)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        self.estimators_ += list(itertools.chain.from_iterable(\n",
    "            t[0] for t in all_results))\n",
    "        self.estimators_features_ += list(itertools.chain.from_iterable(\n",
    "            t[1] for t in all_results))\n",
    "\n",
    "        if self.oob_score:\n",
    "            self._set_oob_score(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_oob_score(self, X, y):\n",
    "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            return column_or_1d(y, warn=True)\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def _get_estimators_indices(self):\n",
    "        # Get drawn indices along both sample and feature axes\n",
    "        for seed in self._seeds:\n",
    "            # Operations accessing random_state must be performed identically\n",
    "            # to those in `_parallel_build_estimators()`\n",
    "            feature_indices, sample_indices = _generate_bagging_indices(\n",
    "                seed, self.bootstrap_features, self.bootstrap,\n",
    "                self.n_features_, self._n_samples, self._max_features,\n",
    "                self._max_samples)\n",
    "\n",
    "            yield feature_indices, sample_indices\n",
    "\n",
    "    @property\n",
    "    def estimators_samples_(self):\n",
    "        \"\"\"\n",
    "        The subset of drawn samples for each base estimator.\n",
    "        Returns a dynamically generated list of indices identifying\n",
    "        the samples used for fitting each member of the ensemble, i.e.,\n",
    "        the in-bag samples.\n",
    "        Note: the list is re-created at each call to the property in order\n",
    "        to reduce the object memory footprint by not storing the sampling\n",
    "        data. Thus fetching the property may be slower than expected.\n",
    "        \"\"\"\n",
    "        return [sample_indices\n",
    "                for _, sample_indices in self._get_estimators_indices()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifier(ClassifierMixin, BaseBagging):\n",
    "    \"\"\"A Bagging classifier.\n",
    "    A Bagging classifier is an ensemble meta-estimator that fits base\n",
    "    classifiers each on random subsets of the original dataset and then\n",
    "    aggregate their individual predictions (either by voting or by averaging)\n",
    "    to form a final prediction. Such a meta-estimator can typically be used as\n",
    "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
    "    tree), by introducing randomization into its construction procedure and\n",
    "    then making an ensemble out of it.\n",
    "    This algorithm encompasses several works from the literature. When random\n",
    "    subsets of the dataset are drawn as random subsets of the samples, then\n",
    "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
    "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
    "    of the dataset are drawn as random subsets of the features, then the method\n",
    "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
    "    on subsets of both samples and features, then the method is known as\n",
    "    Random Patches [4]_.\n",
    "    Read more in the :ref:`User Guide <bagging>`.\n",
    "    .. versionadded:: 0.15\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, default=None\n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "    n_estimators : int, default=10\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, default=1.0\n",
    "        The number of samples to draw from X to train each base estimator (with\n",
    "        replacement by default, see `bootstrap` for more details).\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "    max_features : int or float, default=1.0\n",
    "        The number of features to draw from X to train each base estimator (\n",
    "        without replacement by default, see `bootstrap_features` for more\n",
    "        details).\n",
    "        - If int, then draw `max_features` features.\n",
    "        - If float, then draw `max_features * X.shape[1]` features.\n",
    "    bootstrap : bool, default=True\n",
    "        Whether samples are drawn with replacement. If False, sampling\n",
    "        without replacement is performed.\n",
    "    bootstrap_features : bool, default=False\n",
    "        Whether features are drawn with replacement.\n",
    "    oob_score : bool, default=False\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization error.\n",
    "    warm_start : bool, default=False\n",
    "        When set to True, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit\n",
    "        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
    "        .. versionadded:: 0.17\n",
    "           *warm_start* constructor parameter.\n",
    "    n_jobs : int, default=None\n",
    "        The number of jobs to run in parallel for both :meth:`fit` and\n",
    "        :meth:`predict`. ``None`` means 1 unless in a\n",
    "        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "        processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "    random_state : int or RandomState, default=None\n",
    "        Controls the random resampling of the original dataset\n",
    "        (sample wise and feature wise).\n",
    "        If the base estimator accepts a `random_state` attribute, a different\n",
    "        seed is generated for each instance in the ensemble.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "    n_features_ : int\n",
    "        The number of features when :meth:`fit` is performed.\n",
    "    estimators_ : list of estimators\n",
    "        The collection of fitted base estimators.\n",
    "    estimators_samples_ : list of arrays\n",
    "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
    "        estimator. Each subset is defined by an array of the indices selected.\n",
    "    estimators_features_ : list of arrays\n",
    "        The subset of drawn features for each base estimator.\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        The classes labels.\n",
    "    n_classes_ : int or list\n",
    "        The number of classes.\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "        This attribute exists only when ``oob_score`` is True.\n",
    "    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
    "        Decision function computed with out-of-bag estimate on the training\n",
    "        set. If n_estimators is small it might be possible that a data point\n",
    "        was never left out during the bootstrap. In this case,\n",
    "        `oob_decision_function_` might contain NaN. This attribute exists\n",
    "        only when ``oob_score`` is True.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> from sklearn.ensemble import BaggingClassifier\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> X, y = make_classification(n_samples=100, n_features=4,\n",
    "    ...                            n_informative=2, n_redundant=0,\n",
    "    ...                            random_state=0, shuffle=False)\n",
    "    >>> clf = BaggingClassifier(base_estimator=SVC(),\n",
    "    ...                         n_estimators=10, random_state=0).fit(X, y)\n",
    "    >>> clf.predict([[0, 0, 0, 0]])\n",
    "    array([1])\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
    "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
    "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
    "           1996.\n",
    "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
    "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
    "           1998.\n",
    "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
    "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=False,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=None,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "\n",
    "        super().__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            bootstrap_features=bootstrap_features,\n",
    "            oob_score=oob_score,\n",
    "            warm_start=warm_start,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose)\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
    "        super()._validate_estimator(\n",
    "            default=DecisionTreeClassifier())\n",
    "\n",
    "    def _set_oob_score(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        n_classes_ = self.n_classes_\n",
    "\n",
    "        predictions = np.zeros((n_samples, n_classes_))\n",
    "\n",
    "        for estimator, samples, features in zip(self.estimators_,\n",
    "                                                self.estimators_samples_,\n",
    "                                                self.estimators_features_):\n",
    "            # Create mask for OOB samples\n",
    "            mask = ~indices_to_mask(samples, n_samples)\n",
    "\n",
    "            if hasattr(estimator, \"predict_proba\"):\n",
    "                predictions[mask, :] += estimator.predict_proba(\n",
    "                    (X[mask, :])[:, features])\n",
    "\n",
    "            else:\n",
    "                p = estimator.predict((X[mask, :])[:, features])\n",
    "                j = 0\n",
    "\n",
    "                for i in range(n_samples):\n",
    "                    if mask[i]:\n",
    "                        predictions[i, p[j]] += 1\n",
    "                        j += 1\n",
    "\n",
    "        if (predictions.sum(axis=1) == 0).any():\n",
    "            warn(\"Some inputs do not have OOB scores. \"\n",
    "                 \"This probably means too few estimators were used \"\n",
    "                 \"to compute any reliable oob estimates.\")\n",
    "\n",
    "        oob_decision_function = (predictions /\n",
    "                                 predictions.sum(axis=1)[:, np.newaxis])\n",
    "        oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n",
    "\n",
    "        self.oob_decision_function_ = oob_decision_function\n",
    "        self.oob_score_ = oob_score\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "        The predicted class of an input sample is computed as the class with\n",
    "        the highest mean predicted probability. If base estimators do not\n",
    "        implement a ``predict_proba`` method, then it resorts to voting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        predicted_probabilitiy = self.predict_proba(X)\n",
    "        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n",
    "                                  axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the mean predicted class probabilities of the base estimators in the\n",
    "        ensemble. If base estimators do not implement a ``predict_proba``\n",
    "        method, then it resorts to voting and the predicted class probabilities\n",
    "        of an input sample represents the proportion of estimators predicting\n",
    "        each class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_samples, n_classes)\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        # Check data\n",
    "        X = check_array(\n",
    "            X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False\n",
    "        )\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1}.\"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
    "                             **self._parallel_args())(\n",
    "            delayed(_parallel_predict_proba)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X,\n",
    "                self.n_classes_)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        proba = sum(all_proba) / self.n_estimators\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities for X.\n",
    "        The predicted class log-probabilities of an input sample is computed as\n",
    "        the log of the mean predicted class probabilities of the base\n",
    "        estimators in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_samples, n_classes)\n",
    "            The class log-probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n",
    "            # Check data\n",
    "            X = check_array(\n",
    "                X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "                force_all_finite=False\n",
    "            )\n",
    "\n",
    "            if self.n_features_ != X.shape[1]:\n",
    "                raise ValueError(\"Number of features of the model must \"\n",
    "                                 \"match the input. Model n_features is {0} \"\n",
    "                                 \"and input n_features is {1} \"\n",
    "                                 \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "            # Parallel loop\n",
    "            n_jobs, n_estimators, starts = _partition_estimators(\n",
    "                self.n_estimators, self.n_jobs)\n",
    "\n",
    "            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "                delayed(_parallel_predict_log_proba)(\n",
    "                    self.estimators_[starts[i]:starts[i + 1]],\n",
    "                    self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                    X,\n",
    "                    self.n_classes_)\n",
    "                for i in range(n_jobs))\n",
    "\n",
    "            # Reduce\n",
    "            log_proba = all_log_proba[0]\n",
    "\n",
    "            for j in range(1, len(all_log_proba)):\n",
    "                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n",
    "\n",
    "            log_proba -= np.log(self.n_estimators)\n",
    "\n",
    "            return log_proba\n",
    "\n",
    "        else:\n",
    "            return np.log(self.predict_proba(X))\n",
    "\n",
    "    @if_delegate_has_method(delegate='base_estimator')\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Average of the decision functions of the base classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        score : ndarray of shape (n_samples, k)\n",
    "            The decision function of the input samples. The columns correspond\n",
    "            to the classes in sorted order, as they appear in the attribute\n",
    "            ``classes_``. Regression and binary classification are special\n",
    "            cases with ``k == 1``, otherwise ``k==n_classes``.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Check data\n",
    "        X = check_array(\n",
    "            X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False\n",
    "        )\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1} \"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_decision_function)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        decisions = sum(all_decisions) / self.n_estimators\n",
    "\n",
    "        return decisions\n",
    "\n",
    "\n",
    "class BaggingRegressor(RegressorMixin, BaseBagging):\n",
    "    \"\"\"A Bagging regressor.\n",
    "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
    "    regressors each on random subsets of the original dataset and then\n",
    "    aggregate their individual predictions (either by voting or by averaging)\n",
    "    to form a final prediction. Such a meta-estimator can typically be used as\n",
    "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
    "    tree), by introducing randomization into its construction procedure and\n",
    "    then making an ensemble out of it.\n",
    "    This algorithm encompasses several works from the literature. When random\n",
    "    subsets of the dataset are drawn as random subsets of the samples, then\n",
    "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
    "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
    "    of the dataset are drawn as random subsets of the features, then the method\n",
    "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
    "    on subsets of both samples and features, then the method is known as\n",
    "    Random Patches [4]_.\n",
    "    Read more in the :ref:`User Guide <bagging>`.\n",
    "    .. versionadded:: 0.15\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, default=None\n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "    n_estimators : int, default=10\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, default=1.0\n",
    "        The number of samples to draw from X to train each base estimator (with\n",
    "        replacement by default, see `bootstrap` for more details).\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "    max_features : int or float, default=1.0\n",
    "        The number of features to draw from X to train each base estimator (\n",
    "        without replacement by default, see `bootstrap_features` for more\n",
    "        details).\n",
    "        - If int, then draw `max_features` features.\n",
    "        - If float, then draw `max_features * X.shape[1]` features.\n",
    "    bootstrap : bool, default=True\n",
    "        Whether samples are drawn with replacement. If False, sampling\n",
    "        without replacement is performed.\n",
    "    bootstrap_features : bool, default=False\n",
    "        Whether features are drawn with replacement.\n",
    "    oob_score : bool, default=False\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization error.\n",
    "    warm_start : bool, default=False\n",
    "        When set to True, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit\n",
    "        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
    "    n_jobs : int, default=None\n",
    "        The number of jobs to run in parallel for both :meth:`fit` and\n",
    "        :meth:`predict`. ``None`` means 1 unless in a\n",
    "        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "        processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "    random_state : int or RandomState, default=None\n",
    "        Controls the random resampling of the original dataset\n",
    "        (sample wise and feature wise).\n",
    "        If the base estimator accepts a `random_state` attribute, a different\n",
    "        seed is generated for each instance in the ensemble.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "    n_features_ : int\n",
    "        The number of features when :meth:`fit` is performed.\n",
    "    estimators_ : list of estimators\n",
    "        The collection of fitted sub-estimators.\n",
    "    estimators_samples_ : list of arrays\n",
    "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
    "        estimator. Each subset is defined by an array of the indices selected.\n",
    "    estimators_features_ : list of arrays\n",
    "        The subset of drawn features for each base estimator.\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "        This attribute exists only when ``oob_score`` is True.\n",
    "    oob_prediction_ : ndarray of shape (n_samples,)\n",
    "        Prediction computed with out-of-bag estimate on the training\n",
    "        set. If n_estimators is small it might be possible that a data point\n",
    "        was never left out during the bootstrap. In this case,\n",
    "        `oob_prediction_` might contain NaN. This attribute exists only\n",
    "        when ``oob_score`` is True.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.svm import SVR\n",
    "    >>> from sklearn.ensemble import BaggingRegressor\n",
    "    >>> from sklearn.datasets import make_regression\n",
    "    >>> X, y = make_regression(n_samples=100, n_features=4,\n",
    "    ...                        n_informative=2, n_targets=1,\n",
    "    ...                        random_state=0, shuffle=False)\n",
    "    >>> regr = BaggingRegressor(base_estimator=SVR(),\n",
    "    ...                         n_estimators=10, random_state=0).fit(X, y)\n",
    "    >>> regr.predict([[0, 0, 0, 0]])\n",
    "    array([-2.8720...])\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
    "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
    "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
    "           1996.\n",
    "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
    "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
    "           1998.\n",
    "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
    "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 max_samples=1.0,\n",
    "                 max_features=1.0,\n",
    "                 bootstrap=True,\n",
    "                 bootstrap_features=False,\n",
    "                 oob_score=False,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=None,\n",
    "                 random_state=None,\n",
    "                 verbose=0):\n",
    "        super().__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            bootstrap_features=bootstrap_features,\n",
    "            oob_score=oob_score,\n",
    "            warm_start=warm_start,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict regression target for X.\n",
    "        The predicted regression target of an input sample is computed as the\n",
    "        mean predicted regression targets of the estimators in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        # Check data\n",
    "        X = check_array(\n",
    "            X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False\n",
    "        )\n",
    "\n",
    "        # Parallel loop\n",
    "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
    "                                                             self.n_jobs)\n",
    "\n",
    "        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
    "            delayed(_parallel_predict_regression)(\n",
    "                self.estimators_[starts[i]:starts[i + 1]],\n",
    "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
    "                X)\n",
    "            for i in range(n_jobs))\n",
    "\n",
    "        # Reduce\n",
    "        y_hat = sum(all_y_hat) / self.n_estimators\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
    "        super()._validate_estimator(\n",
    "            default=DecisionTreeRegressor())\n",
    "\n",
    "    def _set_oob_score(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "\n",
    "        predictions = np.zeros((n_samples,))\n",
    "        n_predictions = np.zeros((n_samples,))\n",
    "\n",
    "        for estimator, samples, features in zip(self.estimators_,\n",
    "                                                self.estimators_samples_,\n",
    "                                                self.estimators_features_):\n",
    "            # Create mask for OOB samples\n",
    "            mask = ~indices_to_mask(samples, n_samples)\n",
    "\n",
    "            predictions[mask] += estimator.predict((X[mask, :])[:, features])\n",
    "            n_predictions[mask] += 1\n",
    "\n",
    "        if (n_predictions == 0).any():\n",
    "            warn(\"Some inputs do not have OOB scores. \"\n",
    "                 \"This probably means too few estimators were used \"\n",
    "                 \"to compute any reliable oob estimates.\")\n",
    "            n_predictions[n_predictions == 0] = 1\n",
    "\n",
    "        predictions /= n_predictions\n",
    "\n",
    "        self.oob_prediction_ = predictions\n",
    "        self.oob_score_ = r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant to mark tree leafs\n",
    "cdef SIZE_t TREE_LEAF = -1\n",
    "\n",
    "cdef void _predict_regression_tree_inplace_fast_dense(DTYPE_t *X,\n",
    "                                                      Node* root_node,\n",
    "                                                      double *value,\n",
    "                                                      double scale,\n",
    "                                                      Py_ssize_t k,\n",
    "                                                      Py_ssize_t K,\n",
    "                                                      Py_ssize_t n_samples,\n",
    "                                                      Py_ssize_t n_features,\n",
    "                                                      float64 *out):\n",
    "    \"\"\"Predicts output for regression tree and stores it in ``out[i, k]``.\n",
    "    This function operates directly on the data arrays of the tree\n",
    "    data structures. This is 5x faster than the variant above because\n",
    "    it allows us to avoid buffer validation.\n",
    "    The function assumes that the ndarray that wraps ``X`` is\n",
    "    c-continuous.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DTYPE_t pointer\n",
    "        The pointer to the data array of the input ``X``.\n",
    "        Assumes that the array is c-continuous.\n",
    "    root_node : tree Node pointer\n",
    "        Pointer to the main node array of the :class:``sklearn.tree.Tree``.\n",
    "    value : np.float64_t pointer\n",
    "        The pointer to the data array of the ``value`` array attribute\n",
    "        of the :class:``sklearn.tree.Tree``.\n",
    "    scale : double\n",
    "        A constant to scale the predictions.\n",
    "    k : int\n",
    "        The index of the tree output to be predicted. Must satisfy\n",
    "        0 <= ``k`` < ``K``.\n",
    "    K : int\n",
    "        The number of regression tree outputs. For regression and\n",
    "        binary classification ``K == 1``, for multi-class\n",
    "        classification ``K == n_classes``.\n",
    "    n_samples : int\n",
    "        The number of samples in the input array ``X``;\n",
    "        ``n_samples == X.shape[0]``.\n",
    "    n_features : int\n",
    "        The number of features; ``n_samples == X.shape[1]``.\n",
    "    out : np.float64_t pointer\n",
    "        The pointer to the data array where the predictions are stored.\n",
    "        ``out`` is assumed to be a two-dimensional array of\n",
    "        shape ``(n_samples, K)``.\n",
    "    \"\"\"\n",
    "    cdef Py_ssize_t i\n",
    "    cdef Node *node\n",
    "    for i in range(n_samples):\n",
    "        node = root_node\n",
    "        # While node not a leaf\n",
    "        while node.left_child != TREE_LEAF:\n",
    "            if X[i * n_features + node.feature] <= node.threshold:\n",
    "                node = root_node + node.left_child\n",
    "            else:\n",
    "                node = root_node + node.right_child\n",
    "        out[i * K + k] += scale * value[node - root_node]\n",
    "\n",
    "def _predict_regression_tree_stages_sparse(np.ndarray[object, ndim=2] estimators,\n",
    "                                           object X, double scale,\n",
    "                                           np.ndarray[float64, ndim=2] out):\n",
    "    \"\"\"Predicts output for regression tree inplace and adds scaled value to ``out[i, k]``.\n",
    "    The function assumes that the ndarray that wraps ``X`` is csr_matrix.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t* X_data = <DTYPE_t*>(<np.ndarray> X.data).data\n",
    "    cdef INT32_t* X_indices = <INT32_t*>(<np.ndarray> X.indices).data\n",
    "    cdef INT32_t* X_indptr = <INT32_t*>(<np.ndarray> X.indptr).data\n",
    "\n",
    "    cdef SIZE_t n_samples = X.shape[0]\n",
    "    cdef SIZE_t n_features = X.shape[1]\n",
    "    cdef SIZE_t n_stages = estimators.shape[0]\n",
    "    cdef SIZE_t n_outputs = estimators.shape[1]\n",
    "\n",
    "    # Initialize output\n",
    "    cdef float64* out_ptr = <float64*> out.data\n",
    "\n",
    "    # Indices and temporary variables\n",
    "    cdef SIZE_t sample_i\n",
    "    cdef SIZE_t feature_i\n",
    "    cdef SIZE_t stage_i\n",
    "    cdef SIZE_t output_i\n",
    "    cdef Node *root_node = NULL\n",
    "    cdef Node *node = NULL\n",
    "    cdef double *value = NULL\n",
    "\n",
    "    cdef Tree tree\n",
    "    cdef Node** nodes = NULL\n",
    "    cdef double** values = NULL\n",
    "    safe_realloc(&nodes, n_stages * n_outputs)\n",
    "    safe_realloc(&values, n_stages * n_outputs)\n",
    "    for stage_i in range(n_stages):\n",
    "        for output_i in range(n_outputs):\n",
    "            tree = estimators[stage_i, output_i].tree_\n",
    "            nodes[stage_i * n_outputs + output_i] = tree.nodes\n",
    "            values[stage_i * n_outputs + output_i] = tree.value\n",
    "\n",
    "    # Initialize auxiliary data-structure\n",
    "    cdef DTYPE_t feature_value = 0.\n",
    "    cdef DTYPE_t* X_sample = NULL\n",
    "\n",
    "    # feature_to_sample as a data structure records the last seen sample\n",
    "    # for each feature; functionally, it is an efficient way to identify\n",
    "    # which features are nonzero in the present sample.\n",
    "    cdef SIZE_t* feature_to_sample = NULL\n",
    "\n",
    "    safe_realloc(&X_sample, n_features)\n",
    "    safe_realloc(&feature_to_sample, n_features)\n",
    "\n",
    "    memset(feature_to_sample, -1, n_features * sizeof(SIZE_t))\n",
    "\n",
    "    # Cycle through all samples\n",
    "    for sample_i in range(n_samples):\n",
    "        for feature_i in range(X_indptr[sample_i], X_indptr[sample_i + 1]):\n",
    "            feature_to_sample[X_indices[feature_i]] = sample_i\n",
    "            X_sample[X_indices[feature_i]] = X_data[feature_i]\n",
    "\n",
    "        # Cycle through all stages\n",
    "        for stage_i in range(n_stages):\n",
    "            # Cycle through all trees\n",
    "            for output_i in range(n_outputs):\n",
    "                root_node = nodes[stage_i * n_outputs + output_i]\n",
    "                value = values[stage_i * n_outputs + output_i]\n",
    "                node = root_node\n",
    "\n",
    "                # While node not a leaf\n",
    "                while node.left_child != TREE_LEAF:\n",
    "                    # ... and node.right_child != TREE_LEAF:\n",
    "                    if feature_to_sample[node.feature] == sample_i:\n",
    "                        feature_value = X_sample[node.feature]\n",
    "                    else:\n",
    "                        feature_value = 0.\n",
    "\n",
    "                    if feature_value <= node.threshold:\n",
    "                        node = root_node + node.left_child\n",
    "                    else:\n",
    "                        node = root_node + node.right_child\n",
    "                out_ptr[sample_i * n_outputs + output_i] += (scale\n",
    "                    * value[node - root_node])\n",
    "\n",
    "    # Free auxiliary arrays\n",
    "    free(X_sample)\n",
    "    free(feature_to_sample)\n",
    "    free(nodes)\n",
    "    free(values)\n",
    "\n",
    "\n",
    "def predict_stages(np.ndarray[object, ndim=2] estimators,\n",
    "                   object X, double scale,\n",
    "                   np.ndarray[float64, ndim=2] out):\n",
    "    \"\"\"Add predictions of ``estimators`` to ``out``.\n",
    "    Each estimator is scaled by ``scale`` before its prediction\n",
    "    is added to ``out``.\n",
    "    \"\"\"\n",
    "    cdef Py_ssize_t i\n",
    "    cdef Py_ssize_t k\n",
    "    cdef Py_ssize_t n_estimators = estimators.shape[0]\n",
    "    cdef Py_ssize_t K = estimators.shape[1]\n",
    "    cdef Tree tree\n",
    "\n",
    "    if issparse(X):\n",
    "        if X.format != 'csr':\n",
    "            raise ValueError(\"When X is a sparse matrix, a CSR format is\"\n",
    "                             \" expected, got {!r}\".format(type(X)))\n",
    "        _predict_regression_tree_stages_sparse(estimators, X, scale, out)\n",
    "    else:\n",
    "        if not isinstance(X, np.ndarray) or np.isfortran(X):\n",
    "            raise ValueError(\"X should be C-ordered np.ndarray,\"\n",
    "                             \" got {}\".format(type(X)))\n",
    "\n",
    "        for i in range(n_estimators):\n",
    "            for k in range(K):\n",
    "                tree = estimators[i, k].tree_\n",
    "\n",
    "                # avoid buffer validation by casting to ndarray\n",
    "                # and get data pointer\n",
    "                # need brackets because of casting operator priority\n",
    "                _predict_regression_tree_inplace_fast_dense(\n",
    "                    <DTYPE_t*> (<np.ndarray> X).data,\n",
    "                    tree.nodes, tree.value,\n",
    "                    scale, k, K, X.shape[0], X.shape[1],\n",
    "                    <float64 *> (<np.ndarray> out).data)\n",
    "                ## out += scale * tree.predict(X).reshape((X.shape[0], 1))\n",
    "\n",
    "\n",
    "def predict_stage(np.ndarray[object, ndim=2] estimators,\n",
    "                  int stage,\n",
    "                  object X, double scale,\n",
    "                  np.ndarray[float64, ndim=2] out):\n",
    "    \"\"\"Add predictions of ``estimators[stage]`` to ``out``.\n",
    "    Each estimator in the stage is scaled by ``scale`` before\n",
    "    its prediction is added to ``out``.\n",
    "    \"\"\"\n",
    "    return predict_stages(estimators[stage:stage + 1], X, scale, out)\n",
    "\n",
    "\n",
    "def _random_sample_mask(np.npy_intp n_total_samples,\n",
    "                        np.npy_intp n_total_in_bag, random_state):\n",
    "     \"\"\"Create a random sample mask where ``n_total_in_bag`` elements are set.\n",
    "     Parameters\n",
    "     ----------\n",
    "     n_total_samples : int\n",
    "         The length of the resulting mask.\n",
    "     n_total_in_bag : int\n",
    "         The number of elements in the sample mask which are set to 1.\n",
    "     random_state : RandomState\n",
    "         A numpy ``RandomState`` object.\n",
    "     Returns\n",
    "     -------\n",
    "     sample_mask : np.ndarray, shape=[n_total_samples]\n",
    "         An ndarray where ``n_total_in_bag`` elements are set to ``True``\n",
    "         the others are ``False``.\n",
    "     \"\"\"\n",
    "     cdef np.ndarray[float64, ndim=1, mode=\"c\"] rand = \\\n",
    "          random_state.rand(n_total_samples)\n",
    "     cdef np.ndarray[uint8, ndim=1, mode=\"c\", cast=True] sample_mask = \\\n",
    "          np_zeros((n_total_samples,), dtype=np_bool)\n",
    "\n",
    "     cdef np.npy_intp n_bagged = 0\n",
    "     cdef np.npy_intp i = 0\n",
    "\n",
    "     for i in range(n_total_samples):\n",
    "         if rand[i] * (n_total_samples - i) < (n_total_in_bag - n_bagged):\n",
    "             sample_mask[i] = 1\n",
    "             n_bagged += 1\n",
    "\n",
    "     return sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Values of the error of Prof's with respect to the Truth values varies as follows \n",
      " 1. Mean Squared Error 44.25302099447514 \n",
      " 2.Root Mean Squared Error 6.652294415799345 \n",
      " 3. Mean Absolute Error 4.085524861878453 \n",
      " 4. The R2 Score 0.5585219623674234\n"
     ]
    }
   ],
   "source": [
    "#now to find the original eror between Prof's method and machine learning method\n",
    "mse_pre =  mean_squared_error(data[\"Uob, cp\"].values, data[\"Viscosity @ RC, cp\"].values)\n",
    "mae_pre = mean_absolute_error(data[\"Uob, cp\"].values, data[\"Viscosity @ RC, cp\"].values)\n",
    "r2_pre = r2_score(data[\"Uob, cp\"].values, data[\"Viscosity @ RC, cp\"].values)\n",
    "rmse_pre = sqrt(mse_pre)\n",
    "\n",
    "print(f\"The Values of the error of Prof's with respect to the Truth values varies as follows \\n 1. Mean Squared Error {mse_pre} \\n 2.Root Mean Squared Error {rmse_pre} \\n 3. Mean Absolute Error {mae_pre} \\n 4. The R2 Score {r2_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a copy of the data, rewrite a Gradient boosting algorithm using Grid search and get the \n",
    "#results and use it to do machine learning \n",
    "dataset = data.copy()\n",
    "dataset = dataset.drop(\"Viscosity @ RC, cp\", axis = 1)\n",
    "y = dataset[\"Uob, cp\"].values\n",
    "X = dataset.drop('Uob, cp', axis = 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = GradientBoostingRegressor(n_estimators = 120, random_state= 42)\n",
    "params  = {'n_estimators': np.arange(40, 300, 10), \"learning_rate\": np.arange(0.01, 0.1, 0.01), \"max_depth\" : [2, 3, 4, 5, 6, 7, 8, 9], \"min_samples_leaf\": np.arange(1, 10), \"max_features\": [2, 3, 0.5, 'auto','sqrt', 'log2', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=120,\n",
       "                                                 n_iter_n...\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'max_features': [2, 3, 0.5, 'auto', 'sqrt', 'log2',\n",
       "                                          None],\n",
       "                         'min_samples_leaf': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'n_estimators': array([ 40,  50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160,\n",
       "       170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch = GridSearchCV(estimator=estimator, cv=5, param_grid = params, n_jobs = -1, scoring= \"neg_mean_squared_error\")\n",
    "gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.060000000000000005,\n",
       " 'max_depth': 3,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 3,\n",
       " 'n_estimators': 290}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1266669995766966"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, gridsearch.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.060000000000000005,\n",
       "                          loss='ls', max_depth=3, max_features=3,\n",
       "                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                          min_impurity_split=None, min_samples_leaf=3,\n",
       "                          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                          n_estimators=290, n_iter_no_change=None,\n",
       "                          presort='deprecated', random_state=42, subsample=1.0,\n",
       "                          tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=120,\n",
       "                                                 n_iter_n...\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'max_features': [2, 3, 0.5, 'auto', 'sqrt', 'log2',\n",
       "                                          None],\n",
       "                         'min_samples_leaf': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'n_estimators': array([ 40,  50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160,\n",
       "       170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model in a pickle file\n",
    "file_name = \"grad_boost.pkl\"\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(gridsearch.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Values of the error of Gradient Boosting with respect to the Truth values varies as follows \n",
      " 1. Mean Squared Error 0.1266669995766966 \n",
      " 2.Root Mean Squared Error 0.3559030760989523 \n",
      " 3. Mean Absolute Error 0.23132209008488908 \n",
      " 4. The R2 Score 0.9987363416745513\n"
     ]
    }
   ],
   "source": [
    "#loading and predicting the model\n",
    "gridsearch =  pickle.load(open(\"grad_boost.pkl\", \"rb\"))\n",
    "mse_grid = mean_squared_error(y, gridsearch.predict(X))\n",
    "rmse_grid = sqrt(mse_grid)\n",
    "mae_grid = mean_absolute_error(y, gridsearch.predict(X))\n",
    "r2_grid = r2_score(y, gridsearch.predict(X))\n",
    "print(f\"The Values of the error of Gradient Boosting with respect to the Truth values varies as follows \\n 1. Mean Squared Error {mse_grid} \\n 2.Root Mean Squared Error {rmse_grid} \\n 3. Mean Absolute Error {mae_grid} \\n 4. The R2 Score {r2_grid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             oob_score=False, rand...\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'max_features': [2, 3, 0.5, 'auto', 'sqrt', 'log2',\n",
       "                                          None],\n",
       "                         'min_samples_leaf': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'n_estimators': array([ 40,  50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160,\n",
       "       170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now to prepare a RandomForest regressor\n",
    "params_rf  = {'n_estimators': np.arange(40, 300, 10), \"max_depth\" : [2, 3, 4, 5, 6, 7, 8, 9], \"min_samples_leaf\": np.arange(1, 10), \"max_features\": [2, 3, 0.5, 'auto','sqrt', 'log2', None]}\n",
    "estimator_rf = RandomForestRegressor(random_state = 42)\n",
    "gridsearch_rf = GridSearchCV(estimator=estimator_rf, cv=5, param_grid = params_rf, n_jobs = -1, scoring= \"neg_mean_squared_error\")\n",
    "gridsearch_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 8, 'max_features': 2, 'min_samples_leaf': 1, 'n_estimators': 40}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now to get the best params\n",
    "gridsearch_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model and loading into the disk space\n",
    "file_name = \"random_forest.pkl\"\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(gridsearch_rf.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Values of the error of Random Forest with respect to the Truth values varies as follows \n",
      " 1. Mean Squared Error 0.46617794315047945 \n",
      " 2. Root Mean Squared Error 0.6827722483745802 \n",
      " 3. Mean Absolute Error 0.328511503365855 \n",
      " 4. The R2 Score 0.9953493045467934\n"
     ]
    }
   ],
   "source": [
    "#loading and predicting the model\n",
    "gridsearch_rf =  pickle.load(open(\"random_forest.pkl\", \"rb\"))\n",
    "mse_forest = mean_squared_error(y, gridsearch_rf.predict(X))\n",
    "rmse_forest = sqrt(mse_forest)\n",
    "mae_forest = mean_absolute_error(y, gridsearch_rf.predict(X))\n",
    "r2_forest = r2_score(y, gridsearch_rf.predict(X))\n",
    "print(f\"The Values of the error of Random Forest with respect to the Truth values varies as follows \\n 1. Mean Squared Error {mse_forest} \\n 2. Root Mean Squared Error {rmse_forest} \\n 3. Mean Absolute Error {mae_forest} \\n 4. The R2 Score {r2_forest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f276bf7d850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let us try to visualize the most important features\n",
    "feat_importances = pd.Series(gridsearch_rf.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(6).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the small tree\n",
    "import pydot\n",
    "tree_small = gridsearch_rf.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = X.columns, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "graph.write_png('small_tree.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=AdaBoostRegressor(base_estimator=None, learning_rate=1.0,\n",
       "                                         loss='linear', n_estimators=50,\n",
       "                                         random_state=42),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'learning_rate': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]),\n",
       "                         'loss': ['linear', 'square', 'exponential'],\n",
       "                         'n_estimators': array([ 40,  50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160,\n",
       "       170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now to prepare AdaBoost\n",
    "estimator_ada = AdaBoostRegressor(random_state = 42)\n",
    "params_ada = {'n_estimators': np.arange(40, 300, 10), \"learning_rate\": np.arange(0.01, 0.1, 0.01), \"loss\": [\"linear\", \"square\", \"exponential\"]}\n",
    "gridsearch_ada = GridSearchCV(estimator=estimator_ada, cv=5, param_grid = params_ada, n_jobs = -1, scoring= \"neg_mean_squared_error\")\n",
    "gridsearch_ada.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'loss': 'linear', 'n_estimators': 40}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model and loading into the disk space\n",
    "file_name = \"ada_boost.pkl\"\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(gridsearch_ada.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Values of the error of Adaptive Boosting(AdaBoost) with respect to the Truth values varies as follows \n",
      " 1. Mean Squared Error 5.0497814801721335 \n",
      " 2. Root Mean Squared Error 2.247171884874883 \n",
      " 3. Mean Absolute Error 1.4238424352112078 \n",
      " 4. The R2 Score 0.9496222502274353\n"
     ]
    }
   ],
   "source": [
    "#loading and predicting the model\n",
    "gridsearch_ada =  pickle.load(open(\"ada_boost.pkl\", \"rb\"))\n",
    "mse_ada = mean_squared_error(y, gridsearch_ada.predict(X))\n",
    "rmse_ada = sqrt(mse_ada)\n",
    "mae_ada = mean_absolute_error(y, gridsearch_ada.predict(X))\n",
    "r2_ada = r2_score(y, gridsearch_ada.predict(X))\n",
    "print(f\"The Values of the error of Adaptive Boosting(AdaBoost) with respect to the Truth values varies as follows \\n 1. Mean Squared Error {mse_ada} \\n 2. Root Mean Squared Error {rmse_ada} \\n 3. Mean Absolute Error {mae_ada} \\n 4. The R2 Score {r2_ada}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
